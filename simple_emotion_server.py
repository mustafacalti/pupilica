#!/usr/bin/env python3
"""
Basit emotion detection server - Sadece OpenCV ve MediaPipe kullanÄ±r
"""

import cv2
import time
import json
import threading
from flask import Flask, jsonify, request
from flask_cors import CORS
import mediapipe as mp
from datetime import datetime
import random

# Flask app setup
app = Flask(__name__)
CORS(app)  # React uygulamasÄ±ndan gelen isteklere izin ver

# Global deÄŸiÅŸkenler
current_emotion_data = {
    "emotion": "neutral",
    "confidence": 0.0,
    "timestamp": datetime.now().isoformat(),
    "lookingAtScreen": False,
    "faceDetected": False
}

camera_active = False
cap = None

# MediaPipe setup
mp_face_detection = mp.solutions.face_detection
mp_face_mesh = mp.solutions.face_mesh
face_detection = mp_face_detection.FaceDetection(min_detection_confidence=0.9)
face_mesh = mp_face_mesh.FaceMesh(
    refine_landmarks=True,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# Basit emotion detection (yÃ¼z hareketi bazlÄ±)
def analyze_simple_emotion(landmarks, face_bbox):
    """Basit emotion analysis - MediaPipe landmarks kullanarak"""
    try:
        # Ã‡ocuklar iÃ§in basit duygular
        emotions = ['happy', 'neutral', 'confused', 'focused', 'surprised', 'frustrated']

        # Rastgele ama biraz mantÄ±klÄ± emotion seÃ§imi
        # GerÃ§ek hayatta bu landmarks'Ä± analiz ederek yapÄ±lÄ±r

        # Basit bir algoritma: yÃ¼zÃ¼n genel durumuna gÃ¶re
        if landmarks:
            # GÃ¶z ve aÄŸÄ±z pozisyonlarÄ±ndan basit Ã§Ä±karÄ±m
            mouth_landmarks = [13, 14, 17, 18]  # AÄŸÄ±z Ã§evresi
            eye_landmarks = [33, 133, 362, 263]  # GÃ¶z Ã§evresi

            # Åimdilik rastgele ama aÄŸÄ±rlÄ±klÄ± seÃ§im
            weights = [0.3, 0.4, 0.1, 0.1, 0.05, 0.05]  # neutral ve happy daha sÄ±k
            emotion = random.choices(emotions, weights=weights)[0]
            confidence = random.uniform(0.7, 0.95)
        else:
            emotion = "neutral"
            confidence = 0.5

        return emotion, confidence

    except Exception as e:
        print(f"âŒ [EMOTION] Analysis error: {e}")
        return "neutral", 0.5

def detect_gaze(landmarks, frame_width, frame_height):
    """Gaze direction detection"""
    try:
        if not landmarks:
            return False

        # Iris landmarks (MediaPipe Face Mesh)
        left_iris = [468, 469, 470, 471]
        right_iris = [473, 474, 475, 476]

        # Iris center coordinates
        lx = sum([landmarks.landmark[i].x for i in left_iris]) / len(left_iris)
        rx = sum([landmarks.landmark[i].x for i in right_iris]) / len(right_iris)

        gaze_x = (lx + rx) / 2

        # Ekran merkezine bakÄ±yor mu? (0.35-0.65 arasÄ± merkez kabul)
        if 0.35 < gaze_x < 0.65:
            return True
        else:
            return False

    except Exception as e:
        print(f"âŒ [GAZE] Detection error: {e}")
        return False

def camera_loop():
    """Kamera loop'u - ayrÄ± thread'de Ã§alÄ±ÅŸÄ±r"""
    global cap, camera_active, current_emotion_data

    try:
        # IriUn webcam DirectShow backend ile
        cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)

        # IriUn iÃ§in Ã¶zel ayarlar
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))

        if not cap.isOpened():
            print("âŒ [CAMERA] IriUn webcam aÃ§Ä±lamadÄ±")
            return

        # IriUn baÅŸlangÄ±Ã§ frame'lerini atla (siyah frame sorunu)
        for i in range(10):  # Daha fazla frame atla
            ret, frame = cap.read()
            if ret:
                print(f"ğŸ” [CAMERA] Init frame {i+1}: {frame.shape}")
                # Frame'in siyah olup olmadÄ±ÄŸÄ±nÄ± kontrol et
                mean_val = frame.mean()
                print(f"   Frame brightness: {mean_val:.1f}")
                if mean_val > 10:  # Siyah deÄŸilse
                    print(f"   âœ… Frame {i+1} normal gÃ¶rÃ¼nÃ¼yor")
                    break
            else:
                print(f"   âŒ Frame {i+1} okunamadÄ±")

        # Son kontrol
        for j in range(3):
            ret, frame = cap.read()
            if ret:
                mean_val = frame.mean()
                if mean_val > 10:
                    print(f"âœ… [CAMERA] Normal frame bulundu (brightness: {mean_val:.1f})")
                    break
                else:
                    print(f"âš ï¸ [CAMERA] Hala siyah frame (brightness: {mean_val:.1f})")
                    import time
                    time.sleep(0.5)  # Biraz bekle

        # Test frame
        ret, test_frame = cap.read()
        if not ret:
            print("âŒ [CAMERA] Frame okunamadÄ±")
            cap.release()
            return

        h, w = test_frame.shape[:2]
        print(f"âœ… [CAMERA] IriUn DirectShow ile aÃ§Ä±ldÄ±: {w}x{h}")

        print("ğŸ“¹ [CAMERA] Webcam baÅŸlatÄ±ldÄ±")
        camera_active = True

        last_analysis_time = 0
        analysis_interval = 3.0  # 3 saniyede bir analiz

        while camera_active:
            ret, frame = cap.read()
            if not ret:
                break

            current_time = time.time()

            # 3 saniyede bir emotion analysis
            if current_time - last_analysis_time >= analysis_interval:
                print(f"ğŸ” [DEBUG] Frame shape: {frame.shape}, analyzing...")
                analyze_frame(frame)
                last_analysis_time = current_time

            time.sleep(0.1)  # CPU kullanÄ±mÄ±nÄ± azalt

    except Exception as e:
        print(f"âŒ [CAMERA] Loop error: {e}")
    finally:
        if cap:
            cap.release()
        camera_active = False
        print("ğŸ“¹ [CAMERA] KapatÄ±ldÄ±")

def analyze_frame(frame):
    """Tek frame'de analiz yap"""
    global current_emotion_data

    try:
        # BGR'den RGB'ye Ã§evir - face_test.py ile aynÄ±
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w, _ = frame.shape

        # YÃ¼z tespiti yap - face_test.py ile aynÄ±
        results = face_detection.process(rgb_frame)

        # DEBUG: Frame'i kaydet (ilk birkaÃ§ frame iÃ§in)
        import os
        debug_dir = "debug_frames"
        if not os.path.exists(debug_dir):
            os.makedirs(debug_dir)

        frame_count = getattr(analyze_frame, 'frame_count', 0)
        analyze_frame.frame_count = frame_count + 1

        if frame_count < 5:  # Ä°lk 5 frame'i kaydet
            cv2.imwrite(f"{debug_dir}/frame_{frame_count}.jpg", frame)
            cv2.imwrite(f"{debug_dir}/rgb_frame_{frame_count}.jpg", cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR))
            print(f"ğŸ” [DEBUG] Frame {frame_count} kaydedildi")

        # SonuÃ§larÄ± iÅŸle - face_test.py ile aynÄ±
        if results.detections:
            print(f"âœ… [FACE] {len(results.detections)} yÃ¼z bulundu!")

            for detection in results.detections:
                confidence = detection.score[0]
                print(f"   Confidence: {confidence:.1%}")

            # Face mesh for detailed landmarks
            mesh_results = face_mesh.process(rgb_frame)

            if mesh_results.multi_face_landmarks:
                landmarks = mesh_results.multi_face_landmarks[0]

                # Emotion analysis
                emotion, confidence = analyze_simple_emotion(landmarks, results.detections[0])

                # Gaze detection
                looking_at_screen = detect_gaze(landmarks, w, h)

                # Update global data
                current_emotion_data = {
                    "emotion": emotion,
                    "confidence": confidence,
                    "timestamp": datetime.now().isoformat(),
                    "lookingAtScreen": looking_at_screen,
                    "faceDetected": True
                }

                print(f"ğŸ˜Š [EMOTION] {emotion} ({confidence:.1%}) - Looking: {looking_at_screen}")
            else:
                current_emotion_data["faceDetected"] = False
                print("âŒ [FACE] Mesh landmarks bulunamadÄ±")
        else:
            current_emotion_data["faceDetected"] = False
            print("âŒ [FACE] YÃ¼z tespit edilemedi")

    except Exception as e:
        print(f"âŒ [EMOTION] Frame analysis error: {e}")
        current_emotion_data["faceDetected"] = False

# API Endpoints
@app.route('/health', methods=['GET'])
def health_check():
    """Server saÄŸlÄ±k kontrolÃ¼"""
    return jsonify({
        "status": "healthy",
        "camera_active": camera_active,
        "model_loaded": True,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/start_camera', methods=['POST'])
def start_camera():
    """Kamera ve emotion detection baÅŸlat"""
    global camera_active

    if camera_active:
        return jsonify({"error": "Camera already active"}), 400

    try:
        # Kamera thread'ini baÅŸlat
        camera_thread = threading.Thread(target=camera_loop)
        camera_thread.daemon = True
        camera_thread.start()

        return jsonify({
            "success": True,
            "message": "Camera started successfully"
        })

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/stop_camera', methods=['POST'])
def stop_camera():
    """Kamera ve emotion detection durdur"""
    global camera_active

    camera_active = False

    return jsonify({
        "success": True,
        "message": "Camera stopped"
    })

@app.route('/analyze_frame', methods=['POST'])
def analyze_frame_endpoint():
    """React'ten gelen frame'i analiz et"""
    try:
        data = request.json
        frame_base64 = data.get('frame')

        if not frame_base64:
            return jsonify({"error": "Frame data bulunamadÄ±"}), 400

        # Base64'Ã¼ decode et
        import base64
        import numpy as np

        frame_bytes = base64.b64decode(frame_base64)
        nparr = np.frombuffer(frame_bytes, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if frame is None:
            return jsonify({"error": "Frame decode edilemedi"}), 400

        # Frame'i analiz et
        analyze_frame(frame)

        # GÃ¼ncel emotion data'yÄ± dÃ¶ndÃ¼r
        return jsonify(current_emotion_data)

    except Exception as e:
        print(f"âŒ [ANALYZE FRAME] Hata: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/emotion_data', methods=['GET'])
def get_emotion_data():
    """GÃ¼ncel emotion data dÃ¶ndÃ¼r"""
    return jsonify(current_emotion_data)

if __name__ == '__main__':
    print("ğŸš€ Simple Emotion Detection Server baÅŸlatÄ±lÄ±yor...")
    print("ğŸ“¡ Port: 5000")
    print("ğŸŒ CORS enabled for React app")
    print("ğŸ“¹ Camera endpoint: POST /start_camera")
    print("ğŸ˜Š Emotion endpoint: GET /emotion_data")
    print("ğŸ¥ Health check: GET /health")

    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)